@article{6795963,
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal  = {Neural Computation},
  title    = {Long Short-Term Memory},
  year     = {1997},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  keywords = {},
  doi      = {10.1162/neco.1997.9.8.1735}
}
@article{6795261,
  author   = {Schmidhuber, Jürgen},
  journal  = {Neural Computation},
  title    = {Learning Complex, Extended Sequences Using the Principle of History Compression},
  year     = {1992},
  volume   = {4},
  number   = {2},
  pages    = {234-242},
  keywords = {},
  doi      = {10.1162/neco.1992.4.2.234}
}
@article{LANG199023,
  title    = {A time-delay neural network architecture for isolated word recognition},
  journal  = {Neural Networks},
  volume   = {3},
  number   = {1},
  pages    = {23-43},
  year     = {1990},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(90)90044-L},
  url      = {https://www.sciencedirect.com/science/article/pii/089360809090044L},
  author   = {Kevin J. Lang and Alex H. Waibel and Geoffrey E. Hinton},
  keywords = {Isolated word recognition, Network architecture, Constrained links, Time delays, Multiresolution learning, Multispeaker speech recognition, Neural networks},
  abstract = {A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.}
}
@article{ELMAN1990179,
  title    = {Finding structure in time},
  journal  = {Cognitive Science},
  volume   = {14},
  number   = {2},
  pages    = {179-211},
  year     = {1990},
  issn     = {0364-0213},
  doi      = {https://doi.org/10.1016/0364-0213(90)90002-E},
  url      = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
  author   = {Jeffrey L. Elman},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}
@techreport{robinson1987utility,
  added-at    = {2018-04-29T16:35:16.000+0200},
  author      = {Robinson, A. J. and Fallside, F.},
  biburl      = {https://www.bibsonomy.org/bibtex/286604804e6720be15dac10673b09dff8/nosebrain},
  institution = {Cambridge Univ.},
  interhash   = {3c632364df265d7af795e2defe65894d},
  intrahash   = {86604804e6720be15dac10673b09dff8},
  keywords    = {backpropagation rnn time},
  number      = {CUED/F-INFENG/TR.1},
  owner       = {thierry},
  timestamp   = {2018-04-29T16:35:16.000+0200},
  title       = {The Utility Driven Dynamic Error Propagation Network},
  year        = 1987
}
@article{WERBOS1988339,
  title    = {Generalization of backpropagation with application to a recurrent gas market model},
  journal  = {Neural Networks},
  volume   = {1},
  number   = {4},
  pages    = {339-356},
  year     = {1988},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(88)90007-X},
  url      = {https://www.sciencedirect.com/science/article/pii/089360808890007X},
  author   = {Paul J. Werbos},
  keywords = {Backpropagation, Recurrent, Continuous time, Reinforcement learning, Energy models, Prediction, Modelling, Cerebral cortex},
  abstract = {Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.}
}

@incollection{williams1992gradient,
  author    = {Williams, Ronald J. and Zipser, David},
  title     = {Gradient-based learning algorithms for recurrent networks and their computational complexity},
  booktitle = {Back-propagation: Theory, architectures and applications},
  editor    = {Chauvin, Yves and Rumelhart, David E.},
  publisher = {Erlbaum},
  address   = {Hillsdale, NJ},
  year      = {1992},
  pages     = {433-486}
}

