# Recurrent Neural Network

This project demonstrates a simple recurrent neural network (RNN) trained to predict the next character in a sequence, based on the _Tiny Shakespeare_ dataset.

## RNN Architecture

| Layer  | Type            | Dimensions  | Weight Init. | Notes                                    |
| ------ | --------------- | ----------- | ------------ | ---------------------------------------- |
| Input  | Character data  | vocab       |              |                                          |
| embed  | Embedding       | vocab x 128 | N(0, 1)      |                                          |
| rnn    | LSTM            | 128 x 256   | U(-k, k)     | Two stacked LSTM layers                  |
| Output | Fully Connected | 256 x vocab | U(-l, l)     | one-hot output layer based on vocab size |

where

- k = 1/√hidden_size = 1/√128
- l = √(6/fan_in) = √(6/256)

## Training and Tuning

The _Tiny Shakespeare_ has 40k lines of text from the works of Shakespeare. The model was trained on 80% of those lines, ingesting sequences of 50 characters at a time; the remaining 20% of the lines were reserved for testing.
The network uses cross-entropy loss and was optimized using _Adaptive Moment Estimation_ (Adam) in batches of size 64. The fixed set of training examples was shuffled on each epoch.

The following hyper-parameters were used.

| Hyperparameter | Value |
| -------------- | ----- |
| Epochs         | 10    |
| Batch Size     | 64    |
| Learning Rate  | 0.002 |

## Results

| Metric     | Value  |
| ---------- | ------ |
| Loss       | 1.8673 |
| Perplexity | 6.471  |
| Top-1      | 53.28% |
| Top-5      | 82.66% |
| Top-10     | 91.84% |

## Samples

The user can use the model to generate content by passing the `generate` command line parameter.

The following are some samples of generated content.

```
Enter a seed phrase: JUL
Enter sequence length: 100
Generated text:
JULIET:

Who is it, my master is made a comfort
I pray you, sir, to put you to his country's ruither.
```

```
Enter a seed phrase: ROM
Enter sequence length: 100
Generated text:
ROMEO:



Good morrow, gentle mistress: where is the head of the city
To be revenged for this man in bl
```

## Observations

Exact next-character prediction is extremely low (53.28%) compared to the classification models created during this independent study (>95%). Remarkably, the model still produces reasonably coherent shakespearean prose, demonstrating that it is indeed learning some modest semantics, grammar patterns, etc.

Interestingly, some words generated by the model appear nowhere in the text (e.g., "ruither"), yet, for those unfamiliar with shakespearean english my be convinced that it is a valid word.
