\documentclass[11pt,a4paper]{article}

% Encoding & Fonts
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

% Page layout
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Mathematics
\usepackage{amsmath,amssymb,amsthm}

% Graphics & Tables
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{enumitem}

% Links & References
\usepackage{hyperref}

% Title data
\title{Master's Thesis Proposal\\\Large{Vision-Only Planning Transformer for Unified Trajectory Planning and Control in Autonomous Driving}}
\author{Matthew Evans \\ The University of Texas at Dallas \\ \texttt{matthew.evans@utdallas.edu}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    We propose a Vision-Only Planning Transformer for end-to-end autonomous driving that will unify long-horizon trajectory planning and low-level vehicle control within a single model. Our architecture will process sequences of monocular camera frames and vehicle state, interleaving learnable Planning Tokens—coarse intent representations predicted at fixed intervals—with standard frame tokens. These Planning Tokens will guide subsequent control outputs, mitigating compounding error common in autoregressive policies. We will train the model via imitation learning on CARLA-generated urban driving data to predict both high-level waypoints and instantaneous steering, throttle, and brake commands. In closed-loop evaluation, we expect the Planning-Token–enhanced Transformer to outperform a baseline vision-only model in route completion, trajectory accuracy, and collision avoidance. We also expect attention visualizations to reveal that Planning Tokens focus on distant road features while control tokens attend to immediate obstacles. This work will demonstrate the efficacy of implicit hierarchical planning in a unified vision-only framework.
\end{abstract}


\section*{Introduction}
This proposal presents a Vision-Only Planning Transformer that will unify long-horizon trajectory planning and low-level control within a single end-to-end model. The proposed architecture will process a history of monocular frames and vehicle state, will interleave learnable Planning Tokens—coarse intent representations at fixed intervals—with frame and state tokens, and will output both future waypoints and instantaneous control commands. This dual time-scale prediction is designed to mitigate the compounding error common in purely autoregressive policies \cite{clinton2024planningtransformerlonghorizonoffline}.

Key contributions include:
\begin{itemize}
    \item \textbf{Unified Architecture:} A single Transformer that jointly performs planning and control, eliminating separate modules.
    \item \textbf{Planning Tokens:} High-level intent tokens that will guide long-term behavior without sacrificing responsiveness to immediate scenarios.
    \item \textbf{Reproducibility:} A research prototype using a single front-facing camera and limited compute resources (4× A30 or H100), accompanied by code, trained models, and simulation results.
\end{itemize}

The model will be evaluated in CARLA \cite{DBLP:journals/corr/abs-1711-03938}, where we expect it will achieve higher route completion rates, lower trajectory deviation, and reduced collision frequency compared to a baseline without Planning Tokens. We also anticipate that attention visualizations will demonstrate that Planning Tokens attend to distant landmarks (e.g., upcoming intersections), while control tokens focus on immediate hazards.

\section*{Background and Related Work}
Classical autonomous driving stacks decompose the pipeline into perception, prediction, planning, and control modules. While modularity aids interpretability and safety validation, inter‐module errors accumulate and require extensive hand‐tuning \cite{DBLP:journals/corr/PadenCYYF16,SchwartingWilko2018PaDf}. Early end‐to‐end approaches—e.g., ALVINN \cite{NIPS1988_812b4ba2} and NVIDIA’s PilotNet \cite{DBLP:journals/corr/BojarskiTDFFGJM16}—map raw images directly to steering commands, reducing engineering effort but often failing to generalize due to short‐horizon reasoning.

We will build upon recent Transformer-based models for spatiotemporal driving tasks. For example, TransFuser \cite{chitta2022transfuserimitationtransformerbasedsensor} fuses multi-sensor features for mid-level affordance and trajectory prediction, and ChauffeurNet \cite{DBLP:journals/corr/abs-1812-03079} learns end-to-end vision-based driving policies via imitation. However, these architectures either require additional sensor modalities or focus on short-horizon behaviors rather than jointly modeling long-term planning and immediate control.

In reinforcement learning, the Decision Transformer \cite{DBLP:journals/corr/abs-2106-01345} recasts control as sequence modeling conditioned on desired returns. Building on this, the Planning Transformer \cite{clinton2024planningtransformerlonghorizonoffline} introduces \emph{Planning Tokens}: coarse latent tokens predicted at regular, long‐horizon intervals to capture high‐level intent and guide subsequent low‐level policies, thereby mitigating compounding error in autoregressive control.

This proposal will unify these streams by integrating Planning Tokens into a vision‐only Transformer for driving. We will predict dual time‐scale tokens—long‐horizon intent tokens interleaved with frame and state tokens—to jointly perform trajectory planning and control within one end‐to‐end model, combining the strengths of implicit planning with reactive control in complex traffic scenarios.


\section*{Project Definition and Objectives}
The goal of this project is to develop a vision-only Transformer that unifies trajectory planning and control for autonomous vehicles by integrating long-horizon Planning Tokens \cite{clinton2024planningtransformerlonghorizonoffline}. Unlike modular pipelines, our model will learn implicit planning and reactive control in a single end-to-end network.

Specifically, we aim to achieve the following.
\begin{itemize}
    \item \textbf{Architecture Design:} Extend a multi-head self-attention Transformer to process sequences of image frames and vehicle state, predicting Planning Tokens at coarse intervals alongside low-level control outputs.
    \item \textbf{Implementation:} Build and train the model using the CARLA simulator \cite{DBLP:journals/corr/abs-1711-03938}, leveraging its diverse urban scenarios for robust evaluation.
    \item \textbf{Evaluation:} Compare closed-loop driving performance—measured by success rate, trajectory deviation, and collision rate—against a baseline vision-only Transformer without Planning Tokens.
    \item \textbf{Interpretability:} Analyze attention patterns to validate that Planning Tokens attend to long-range road geometry, while control tokens focus on immediate hazards.
    \item \textbf{Reproducibility:} Release code, trained weights, and detailed documentation to facilitate follow-up research.
\end{itemize}

\section*{Methodology}
Our expected approach is given as follows.

\begin{itemize}
    \item \textbf{Data Collection \& Preprocessing:}
          We use the CARLA simulator \cite{DBLP:journals/corr/abs-1711-03938} to record sequences of monocular frames (at 10 Hz) and vehicle states (speed, heading). Frames are resized to \(128\times128\) and normalized; state vectors are linearly projected to the same embedding dimension.

    \item \textbf{Tokenization \& Embedding:}
          Each frame and state at time \(t\) becomes a token embedding with added sinusoidal positional encodings. At fixed intervals (\(K\) timesteps), we insert learnable \emph{Planning Tokens} as in \cite{clinton2024planningtransformerlonghorizonoffline} to capture coarse, long-horizon intent.

    \item \textbf{Transformer Backbone:}
          We stack \(L\) layers of multi-head self-attention and feed-forward sublayers \cite{DBLP:journals/corr/VaswaniSPUJGKP17}, operating on the mixed sequence of frame, state, and Planning Tokens.
          \begin{itemize}
              \item \emph{Planning-Token Head}: MLP predicting latent intent vectors at each Planning Token position.
              \item \emph{Control-Token Head}: MLP producing steering, throttle, and brake commands at each regular token.
          \end{itemize}

    \item \textbf{Training Regime:}
          We perform imitation learning on expert demonstrations, minimizing a combined loss:
          \[
              \mathcal{L} = \lambda_{\text{ctrl}}\,\mathrm{MSE}(\hat{u},u^*) \;+\;
              \lambda_{\text{plan}}\,\mathrm{MSE}(\hat{p},p^*)
          \]
          where \(\hat{u}\) are predicted controls, \(u^*\) ground-truth controls, \(\hat{p}\) predicted Planning Tokens, and \(p^*\) coarse future waypoints clustered every \(K\) steps. Optimization uses AdamW with cosine-annealed learning rate.

    \item \textbf{Evaluation:}
          In closed-loop CARLA scenarios, we measure success rate, trajectory deviation, and collision rate. We compare against a baseline vision-only Transformer without Planning Tokens to quantify the impact of hierarchical intent guidance.
\end{itemize}

\section*{Evaluation Plan}

We will rigorously assess our Vision-Only Planning Transformer in closed-loop driving tasks using the CARLA simulator \cite{DBLP:journals/corr/abs-1711-03938}, focusing on how Planning Tokens improve long-horizon behavior and robustness.

\begin{enumerate}
    \item \textbf{Closed-Loop Benchmarks:}
          \begin{itemize}
              \item \emph{Routes:} Three predefined urban routes of increasing length (1 km, 2 km, 5 km), incorporating intersections, turns, and traffic lights.
              \item \emph{Traffic Density:} Low (5 vehicles), medium (15 vehicles), and high (30 vehicles) randomized traffic agents.
          \end{itemize}
          We record \emph{success rate} (route completion), \emph{trajectory deviation} (mean lateral and longitudinal error), and \emph{collision rate}.

    \item \textbf{Baseline Comparison:}
          Compare against a identical vision-only Transformer without Planning Tokens to quantify the benefit of hierarchical intent guidance.

    \item \textbf{Ablation Studies:}
          Vary Planning Token interval \(K\in\{5,10,20\}\) to study the trade-off between long-horizon guidance and model complexity, measuring performance metrics above and compounding error reduction as in \cite{clinton2024planningtransformerlonghorizonoffline}.

    \item \textbf{Attention Visualization:}
          Generate attention maps to verify that:
          \begin{itemize}
              \item Planning Tokens attend to distant landmarks (e.g., upcoming intersections).
              \item Control tokens attend to nearby obstacles and lane markings.
          \end{itemize}

    \item \textbf{Statistical Significance:}
          For each metric, run 30 trials per scenario and perform paired \(t\)-tests (\(\alpha=0.05\)) to validate significant improvements when using Planning Tokens.
\end{enumerate}


\section*{Implementation Feasibility and Timeline}

The proposed work leverages existing open-source tools (CARLA, PyTorch) and UTD's HPC cluster to ensure rapid development and reproducibility. Below is a six-month plan:

\begin{tabular}{p{3cm}p{10cm}}
    \toprule
    \textbf{Months} & \textbf{Tasks}                                                                  \\
    \midrule
    1--2            & \begin{itemize}[nosep]
                          \item Set up CARLA data pipelines; collect and preprocess image/state sequences.
                          \item Implement tokenization with interleaved Planning Tokens.
                      \end{itemize} \\

    3--4            & \begin{itemize}[nosep]
                          \item Build Transformer backbone and dual heads.
                          \item Train baseline and Planning-Token model.
                      \end{itemize}                                 \\

    5               & \begin{itemize}[nosep]
                          \item Conduct closed-loop evaluations, ablations on Planning Token interval.
                          \item Generate attention visualizations.
                      \end{itemize}     \\

    6               & \begin{itemize}[nosep]
                          \item Statistical analysis and significance testing.
                          \item Documentation, code release, and thesis writing.
                      \end{itemize}                           \\
    \bottomrule
\end{tabular}

\vspace{1em}

\noindent\textbf{Feasibility:}
\begin{itemize}[nosep]
    \item \emph{Compute:} UTD’s Juno cluster provides ample GPU capacity for both training and evaluation.
    \item \emph{Software:} CARLA and PyTorch ecosystems support rapid iteration and simulation-in-the-loop testing.
    \item \emph{Expertise:} The IRVL research team's prior experience with learning perception and robotic simulation ensures timely progress.
\end{itemize}


\section*{Expected Outcomes and Deliverables}
Upon project completion, we will deliver:

\begin{itemize}
    \item \textbf{Code Repository:} Well-documented PyTorch implementation of the Vision-Only Planning Transformer with Planning Tokens and baseline model.
    \item \textbf{Trained Models:} Checkpoints for both the Planning-Token and non–Planning-Token variants, including scripts for inference in CARLA.
    \item \textbf{Evaluation Report:} Detailed metrics (success rates, trajectory deviation, collision rates) across scenarios, ablation results on token interval, and statistical significance analyses.
    \item \textbf{Attention Visualizations:} Heatmaps demonstrating high-level Planning Tokens attending to long-range landmarks and low-level control tokens focusing on immediate hazards.
    \item \textbf{Thesis Document \& Presentation:} Concise write-up of methodology, results, and insights; slides for defense and a possible conference submission.
\end{itemize}

\section*{Conclusion}

This proposal outlines the development of a Vision-Only Planning Transformer that will unify long-horizon trajectory planning and low-level control in a single end-to-end model. By interleaving learnable Planning Tokens with frame and state embeddings, we expect to mitigate compounding error and achieve superior route completion, reduced trajectory deviation, and lower collision rates in dense urban scenarios compared to a baseline without hierarchical intent guidance. We will validate this through closed-loop CARLA simulations and statistical analyses, and anticipate that attention visualizations will reveal Planning Tokens attending to distant landmarks while control tokens focus on immediate hazards. Successful completion of this work will demonstrate the practicality of implicit hierarchical planning in vision-only architectures and pave the way for extensions to multi-camera setups and real-world vehicle deployment.

\bibliographystyle{unsrt}
\bibliography{references}


\end{document}